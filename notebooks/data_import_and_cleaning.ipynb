{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "import pdfplumber\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "\n",
    "PATH = Path.cwd().parent.joinpath('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "The goal of this notebook is to get the data on vessel ownership ready for analysis, for instance, with Neo4J.\n",
    "\n",
    "\n",
    "### Data sources\n",
    "1. List of vessels in the shadow fleet, compiled by the Kiev School of Economics Institute. \n",
    "2. Vessel information (including ownership) obtained from the GFW API, using IMO numbers.\n",
    "3. Event data from GFW API (port visits, loitering, ais).\n",
    "4. The same data, but manually downloaded from the GFW platform.\n",
    "5. Vessel tracks of shadow fleet vessels, manually downloaded from GFW.\n",
    "6. Vessel history information downloaded as pdfs from Equasis.\n",
    "7. Company assets downloaded as pdfs from Equasis.\n",
    "\n",
    "### Possible data issues\n",
    "\n",
    "1. Completeness: we don't know if the data provided by KSE is complete. The definition used by KSE of the shadowfleet is: \"In this paper we define the shadow fleet as consisting of non-G7/EU owned or managed vessels navigating without International Group (IG) protection and indemnity (P&I) insurance.\" This definition is debatable. Also the exact dates on which insurance was in place, is not known, only the months, which is rather imprecise. This should be double checked if the issue becomes important. We should also be careful with the AIS off switching data from GFW. It's an experimental feature and it only works in marine environments that are not to crowded, due to signal processing limitations of AIS receivers. \n",
    "2. Correct: Vessels change owners regularly and the KSE data doesn't always seem to reflect the most current ownership data. This ownership data can be ammended with data from Equasis and Global Fishing Watch, but these last sources also don't always agree or don't show the ultimate benicial owners. The sanction regime is also changing, so some vessels are sanctioned, while this is not reflected in the KSE data. This should be manually updated. Due to changes in ownership and the covert actions of some vessels, we have to assume that AIS data isn't always correct. Positions can be spoofed and AIS gaps can be attributed to changes in ownership (and often accompanying MMSI changes) or reception issues, especially in crowded marine environments like the North Sea and the Straight of Gibraltar. \n",
    "\n",
    "### Method\n",
    "\n",
    "1. Create a dataframe with shadowfleet vessels and create a date range for each vessel when it's presumed to be uninsured or underinsured.\n",
    "2. Query Global Fishing Watch to obtain information on identity (MMSIs for instance). \n",
    "3. Query Global Fishing Watch to obtain loitering, port visits and ais off switching events.\n",
    "4. Parse Equasis data on vessels and companies and extract vessel and company specifics and history.\n",
    "5. Perform some exploratory data analsysis to identify problems and possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean shadowfleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of relevant vessels\n",
    "\n",
    "kse = pd.read_csv(PATH.joinpath('processed', 'shadowfleet_kse.csv'))\n",
    "len(kse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import vessel data from GFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from API downloads\n",
    "\n",
    "vessels, owners = sf.get_vessels(query=kse.imo.unique(),\n",
    "                                 filename=PATH.joinpath('raw_gfw', 'api', 'vessels.json'),\n",
    "                                 field='imo',\n",
    "                                 limit=10)\n",
    "\n",
    "print(f'Imported {len(vessels)} vessels and {len(owners)} owners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data\n",
    "\n",
    "vessels = []\n",
    "owners = []\n",
    "\n",
    "with open(PATH.joinpath('raw_gfw', 'api', 'vessels.json'), 'r') as lines:\n",
    "    for line in lines:\n",
    "        for records in literal_eval(line).get('entries'):\n",
    "            for vessel in records.get('selfReportedInfo'):\n",
    "                vessel.update({'query': literal_eval(line).get('query')})\n",
    "                vessels.append(vessel)\n",
    "                ssvid = vessel.get('ssvid')\n",
    "\n",
    "                coms = records.get('registryOwners')\n",
    "                if len(coms) > 0:\n",
    "                    for com in coms:\n",
    "                        com.update({'query': literal_eval(line).get('query'), 'mmsi': ssvid})\n",
    "                        owners.append(com)\n",
    "\n",
    "df_vessels = pd.DataFrame(vessels)\n",
    "\n",
    "# And clean\n",
    "df_vessels = df_vessels[df_vessels.imo.notna()].copy()\n",
    "df_vessels = df_vessels.astype({'imo': 'int', 'ssvid': 'int'})\n",
    "df_vessels = df_vessels[df_vessels.imo.isin(kse.imo)].copy()\n",
    "len(df_vessels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract vessel data equasis using PDF plumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in PATH.joinpath('equasis', 'vessels').glob('*.pdf'):\n",
    "    pdf = pdfplumber.open(file)    \n",
    "    \n",
    "    # Get imo\n",
    "    for lines in pdf.pages[0].extract_text_lines():\n",
    "        if isinstance(lines, dict):\n",
    "            if 'imo: ' in lines.get('text'):\n",
    "                imo = lines.get('text').replace('imo: ', '')\n",
    "\n",
    "    # Get tables\n",
    "    for page in pdf.pages:\n",
    "        tables = page.extract_tables()\n",
    "\n",
    "        for table in tables:\n",
    "            tabs = []\n",
    "            keys = table[0]\n",
    "            keys = [x.replace('\\n', ' ') for x in keys]\n",
    "            for tab in table[1:]:\n",
    "                values = [str(x).replace('\\n', ' ') for x in tab]\n",
    "                tab = dict(zip(keys, values))\n",
    "                tabs.append(tab)\n",
    "            df = pd.DataFrame(tabs)\n",
    "            df['imo'] = imo\n",
    "            dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create relevant dataframes\n",
    "\n",
    "companies = []\n",
    "inspections = []\n",
    "human = []\n",
    "names = []\n",
    "flags = []\n",
    "classifications = []\n",
    "current_companies = []\n",
    "current_info = []\n",
    "\n",
    "\n",
    "for df in dfs:\n",
    "    if 'Company' in df.columns:\n",
    "        companies.append(df)\n",
    "    elif 'Detention' in df.columns:\n",
    "        inspections.append(df)\n",
    "    elif 'Human element deficiencies' in df.columns:\n",
    "        human.append(df)\n",
    "    elif 'Name of ship' in df.columns:\n",
    "        names.append(df)\n",
    "    elif 'Flag' in df.columns and 'Date of effect' in df.columns:\n",
    "        flags.append(df)\n",
    "    elif 'Classification society' and 'Date of survey' in df.columns:\n",
    "        classifications.append(df)\n",
    "    elif 'Name of company' and 'Address' in df.columns:\n",
    "        current_companies.append(df)\n",
    "\n",
    "\n",
    "\n",
    "companies = pd.concat(companies)\n",
    "inspections = pd.concat(inspections)\n",
    "human = pd.concat(human)\n",
    "names = pd.concat(names)\n",
    "flags = pd.concat(flags)\n",
    "classifications = pd.concat(classifications)\n",
    "current_companies = pd.concat(current_companies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataframes\n",
    "\n",
    "def clean_dates(df):\n",
    "    cols = [x for x in df.columns if 'date' in x.lower()]\n",
    "    to_replace = {'during ': '', 'since ': '', 'before ': ''}\n",
    "    for col in cols: \n",
    "        df[col] = df[col].replace(to_replace, regex=True)\n",
    "        df[col] = pd.to_datetime(df[col], format='mixed', dayfirst=True, errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "# First clean up the companies dataframe\n",
    "companies.drop('Sources', axis=1, inplace=True)\n",
    "companies.columns = ['company', 'role', 'start_date', 'imo']\n",
    "companies.role = companies.role.str.replace(' T', '').str.replace('manager/', 'manager /').str.replace('/Com', '/ Com')\n",
    "companies = clean_dates(companies)\n",
    "companies.imo = companies.imo.astype(int)\n",
    "\n",
    "# Create end date column\n",
    "companies = companies.sort_values(by=['imo', 'role', 'start_date'])\n",
    "companies['end_date'] = companies.groupby(['imo', 'role'])['start_date'].shift(-1)\n",
    "\n",
    "\n",
    "# Clean up names dataframe\n",
    "names = clean_dates(names)\n",
    "names.columns = ['vessel_name', 'start_date', 'source', 'imo']\n",
    "names = names.sort_values(by=['imo', 'start_date'])\n",
    "names['end_date'] = names.groupby('imo')['start_date'].shift(-1)\n",
    "names.imo = names.imo.astype(int)\n",
    "\n",
    "flags = clean_dates(flags)\n",
    "flags.columns = ['flag', 'start_date', 'source', 'imo']\n",
    "flags = flags.sort_values(by=['imo', 'start_date'])\n",
    "flags['end_date'] = flags.groupby('imo')['start_date'].shift(-1)\n",
    "flags.imo = flags.imo.astype(int)\n",
    "\n",
    "classifications = clean_dates(classifications)\n",
    "classifications.columns = ['classification_society', 'date_of_survey', 'source', 'imo']\n",
    "classifications.imo = classifications.imo.astype(int)\n",
    "\n",
    "inspections = clean_dates(inspections)\n",
    "inspections.columns = ['authority', 'port', 'date', 'detention', 'PSC_organisation', \n",
    "                       'inspection_type', 'duration', 'number_of_deficiencies', 'imo']\n",
    "inspections = inspections.reset_index().sort_values(['imo', 'index'])\n",
    "inspections.authority = inspections.authority.ffill()\n",
    "inspections['port'] = inspections['port'].ffill()\n",
    "inspections['date'] = inspections['date'].ffill()\n",
    "inspections['detention'] = inspections['detention'].ffill()\n",
    "inspections.drop('index', axis=1, inplace=True)\n",
    "inspections.imo = inspections.imo.astype(int)\n",
    "\n",
    "current_companies = clean_dates(current_companies)\n",
    "current_companies.columns = ['company_imo', 'role', 'company', 'address', 'start_date', 'imo']\n",
    "current_companies.role = current_companies.role.str.replace(' T', '')\\\n",
    "        .str.replace('manager/', 'manager /')\\\n",
    "        .str.replace('/Com', '/ Com')\n",
    "current_companies.imo = current_companies.imo.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "\n",
    "dataframes = [companies, names, flags, classifications, inspections, current_companies]\n",
    "df_names = ['companies', 'names', 'flags', 'classifications', 'inspections', 'current_companies']\n",
    "\n",
    "for df_, name in zip(dataframes, df_names):\n",
    "    df_.to_csv(PATH.joinpath('processed', f'owners_{name}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract company info using pdf plumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15292"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for file in PATH.joinpath('equasis', 'companies').glob('*.pdf'):\n",
    "    pdf = pdfplumber.open(file)    \n",
    "\n",
    "    info = pdf.pages[0].extract_tables()[0][1][0].split(' : ')\n",
    "    imo = info[1].replace('\\nName of company', '')\n",
    "    name = info[2].replace('\\nAddress', '')\n",
    "    address = info[3].replace('\\nLast update', '').replace('C/O: ', '')\n",
    "    last_update = info[4]\n",
    "\n",
    "\n",
    "    # Get tables\n",
    "    for page in pdf.pages:\n",
    "        tables = page.extract_tables()\n",
    "\n",
    "        for table in tables:\n",
    "            if 'Gross\\ntonnage' in table[0]:\n",
    "            \n",
    "                tabs = []\n",
    "                keys = table[0]\n",
    "                keys = [x.replace('\\n', ' ') for x in keys]\n",
    "                for tab in table[1:]:\n",
    "                    values = [str(x).replace('\\n', ' ') for x in tab]\n",
    "                    tab = dict(zip(keys, values))\n",
    "                    tabs.append(tab)\n",
    "                df = pd.DataFrame(tabs)\n",
    "                df['company_imo'] = imo\n",
    "                df['name'] = name\n",
    "                df['address'] = address\n",
    "                df['last_update'] = last_update\n",
    "                dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs).reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "df.last_update = pd.to_datetime(df.last_update, format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "def extract_date(text):\n",
    "    match = re.search(r'\\d{2}/\\d{2}/\\d{4}|\\d{4}', text)\n",
    "    return match.group(0) if match else None\n",
    "def extract_text(text):\n",
    "    match = re.split(r'\\(', text)[0].strip()\n",
    "    return match if match else None\n",
    "\n",
    "\n",
    "df['since'] = df['acting_as_(since)'].apply(extract_date)\n",
    "df['role'] = df['acting_as_(since)'].apply(extract_text)\n",
    "df = df[(df.imo.notna()) & (df.since.notna())].copy()\n",
    "df.since = df.since.apply(lambda x: '01/01/' + x if len(x) < 5 else x)\n",
    "df.since = pd.to_datetime(df.since, format='%d/%m/%Y', errors='coerce')\n",
    "df.drop('acting_as_(since)', axis=1, inplace=True)\n",
    "\n",
    "#df.year_of_build = df.year_of_build.str.replace('', 0)\n",
    "#df[['imo', 'company_imo']] = df[['imo', 'company_imo']].astype(int)\n",
    "\n",
    "df.year_of_build = df.year_of_build.apply(lambda x: int(x) if x else 0)\n",
    "df.imo = df.imo.apply(lambda x: int(x) if x else 0)\n",
    "df[['imo', 'company_imo', 'year_of_build']] = df[['imo', 'company_imo', 'year_of_build']].astype(int)\n",
    "\n",
    "df.to_csv(PATH.joinpath('processed', 'owners_companies_details_vessels.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract just company information, not vessels\n",
    "\n",
    "companies = []\n",
    "for file in PATH.joinpath('equasis', 'companies').glob('*.pdf'):\n",
    "    pdf = pdfplumber.open(file)    \n",
    "\n",
    "    info = pdf.pages[0].extract_tables()[0][1][0].split(' : ')\n",
    "    \n",
    "    companies.append({'imo': info[1].replace('\\nName of company', ''),\n",
    "                        'name': info[2].replace('\\nAddress', ''),\n",
    "                        'address': info[3].replace('\\nLast update', '').replace('C/O: ', ''),\n",
    "                        'last_update': info[4]})\n",
    "    \n",
    "equasis_records = pd.DataFrame(companies)\n",
    "len(equasis_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "\n",
    "\n",
    "countries = ['Netherlands', 'Belgium', 'Germany', 'United Kingdom', 'France', 'Spain', 'Portugal', 'Italy', \n",
    "'Greece', 'Turkey', 'Marshall Islands', 'United Arab Emirates', 'Saudi Arabia', 'Russia', 'China', 'Japan',\n",
    "'South Korea', 'Taiwan', 'Philippines', 'Indonesia', 'Australia', 'New Zealand', 'Spain', 'Norway', 'Denmark',\n",
    "'Sweden', 'Finland', 'Belarus', 'Turkey', 'Egypt', 'South Africa', 'Brazil', 'Argentina', 'Chile', 'Peru', \n",
    "'Panama', 'Cook Islands', 'Bermuda', 'Cayman Islands', 'Bahamas', 'Honduras', 'Costa Rica', 'Nicaragua', 'British Virgin Islands',\n",
    "'Seychelles', 'Mauritius', 'Madagascar', 'Mozambique', 'Tanzania', 'Kenya', 'Somalia', 'Djibouti', 'Yemen', 'Oman', 'India',\n",
    "'Liberia', 'Switzerland', 'Singapore', 'Hong Kong', 'Vietnam', 'Thailand', 'Malaysia', 'Bangladesh', 'Sri Lanka', 'Pakistan',\n",
    "'Antigua & Barbuda', 'St Kitts & Nevis', 'St Vincent & Grenadines', 'Grenada', 'St Lucia', 'Barbados', 'Trinidad & Tobago',\n",
    "'Greece', 'Cyprus', 'Malta', 'Lebanon', 'United Ara Emirates', 'Georgia', 'Isle of Man', 'Jersey', 'Guernsey', 'Gibraltar',\n",
    "'Latvia', 'Lithuania', 'Estonia', 'Poland', 'Ukraine', 'Romania', 'Bulgaria', 'Croatia', 'Slovenia', 'Bosnia & Herzegovina',\n",
    "'Kazakhstan', 'Uzbekistan', 'Kyrgyzstan', 'Tajikistan', 'Turkmenistan', 'Afghanistan', 'Iran', 'Iraq',\n",
    "'Nigeria', 'Gabon', 'Equatorial Guinea', 'Cameroon', 'Congo', 'Angola', 'Namibia', 'Botswana', 'Zimbabwe', 'Zambia', 'Malawi',\n",
    "'Azerbaijan', 'Armenia', 'Moldova', 'Montenegro', 'Serbia', 'Kosovo', 'Albania', 'North Macedonia', 'Hungary', 'Slovakia',\n",
    "'Ireland', 'Canada', 'Canary Islands', 'Greenland', 'Iceland', 'Faroe Islands', 'Puerto Rico', 'Dominican Republic',\n",
    "'Montenegro', 'USA', 'Monaco', 'Kuwait', 'Libya' ]\n",
    "\n",
    "def extract_country(address):\n",
    "    for country in countries:\n",
    "        if country in address:\n",
    "            return country\n",
    "    return None\n",
    "\n",
    "equasis_records.address = equasis_records.address.str.replace('\\n', ' ')\n",
    "equasis_records['country'] = equasis_records.address.apply(extract_country)\n",
    "\n",
    "equasis_records.head()\n",
    "equasis_records.rename(columns={'imo': 'company_imo'}, inplace=True)\n",
    "equasis_records['key'] = equasis_records.name.str.replace(' ', '').str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1678"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_company_list = pd.read_csv(PATH.joinpath('processed', 'equasis_company_details.csv'))\n",
    "df_company_list['key'] = df_company_list.company.str.replace(' ', '').str.upper().str.strip()\n",
    "df_company_list.company_imo.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5083"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_vessels = pd.read_csv(PATH.joinpath('processed', 'owners_companies.csv'))\n",
    "company_vessels = company_vessels[(company_vessels.end_date >= '2022-01-01') | (company_vessels.end_date.isna())].copy().reset_index(drop=True)\n",
    "company_vessels.replace({'NNK- KAMCHATNEFTEPRODUK JSC': 'NNK-KAMCHATNEFTEPRODUKT JSC'}, inplace=True)\n",
    "company_vessels['key'] = company_vessels.company.str.replace(' ', '').str.upper().str.strip()\n",
    "len(company_vessels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(company_vessels, equasis_records, on='key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_imo</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>last_update</th>\n",
       "      <th>country</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>5138401</td>\n",
       "      <td>NNK-KAMCHATNEFTEPRODUKT JSC</td>\n",
       "      <td>ul Kosmonavtov 1, Petropavlovsk-Kamchatskiy, K...</td>\n",
       "      <td>01/10/2024</td>\n",
       "      <td>Russia</td>\n",
       "      <td>NNK-KAMCHATNEFTEPRODUKTJSC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     company_imo                         name  \\\n",
       "1654     5138401  NNK-KAMCHATNEFTEPRODUKT JSC   \n",
       "\n",
       "                                                address last_update country  \\\n",
       "1654  ul Kosmonavtov 1, Petropavlovsk-Kamchatskiy, K...  01/10/2024  Russia   \n",
       "\n",
       "                             key  \n",
       "1654  NNK-KAMCHATNEFTEPRODUKTJSC  "
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equasis_records[equasis_records.name.str.contains('KAMCHATNEFTE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company\n",
       "UNKNOWN                       273\n",
       "K&O SHIPMANAGEMENT FZE         17\n",
       "DENIZCILIK VE GEMI             13\n",
       "MEDITERRANEANMASTE FREIGHT      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[merged.company_imo.isna()].company.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(PATH.joinpath('processed', 'company_vessels_final.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with uninsured/underinsured ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create uninsured dataframe with start and end dates of uninsured periods and sanctions\n",
    "\n",
    "rows = []\n",
    "cols = [col for col in kse.columns if '202' in col]\n",
    "for i, row in kse.iterrows():\n",
    "    for col in cols[3:]:\n",
    "        if pd.isnull(row[col]):\n",
    "            continue\n",
    "        else:\n",
    "            record = {'imo': row.imo,\n",
    "                      'start_date': f'01-{col[:-5]}-{col[-4:]}'}\n",
    "            rows.append(record)\n",
    "\n",
    "uninsured = pd.DataFrame(rows)\n",
    "uninsured.start_date = pd.to_datetime(uninsured.start_date, \n",
    "                                      format='%d-%m-%Y', \n",
    "                                      dayfirst=True)\n",
    "uninsured['end_date'] = pd.to_datetime(uninsured['start_date'], \n",
    "                                       format=\"%Y%m\") + pd.tseries.offsets.MonthEnd(0)\n",
    "\n",
    "# Create date ranges\n",
    "uninsured = uninsured.groupby('imo').agg({'start_date': 'min', \n",
    "                                          'end_date': 'max'}).reset_index()\n",
    "\n",
    "# And add sanction date to the uninsured dataframe\n",
    "\n",
    "uninsured = pd.merge(uninsured, \n",
    "                     kse[['imo', 'earliest_sanction_date']], \n",
    "                     on='imo', \n",
    "                     how='left')\n",
    "\n",
    "uninsured.to_csv(PATH.joinpath('processed', 'uninsured.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find activity in ship to ship transfer areas\n",
    "\n",
    "Import csv files, filter on time period and area and write to parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Malta</td>\n",
       "      <td>POLYGON ((14.11697 36.12737, 14.11697 35.69324...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laconia</td>\n",
       "      <td>POLYGON ((22.57003 36.84774, 22.37132 36.48697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Augusta</td>\n",
       "      <td>POLYGON ((15.00922 37.52169, 15.00922 37.12402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dakar</td>\n",
       "      <td>POLYGON ((-17.68226 14.78504, -17.68226 13.659...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lome</td>\n",
       "      <td>POLYGON ((1.80680 6.13829, 0.19884 6.13829, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name                                           geometry\n",
       "0    Malta  POLYGON ((14.11697 36.12737, 14.11697 35.69324...\n",
       "1  Laconia  POLYGON ((22.57003 36.84774, 22.37132 36.48697...\n",
       "2  Augusta  POLYGON ((15.00922 37.52169, 15.00922 37.12402...\n",
       "3    Dakar  POLYGON ((-17.68226 14.78504, -17.68226 13.659...\n",
       "4     Lome  POLYGON ((1.80680 6.13829, 0.19884 6.13829, 0...."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_locations = gpd.read_file(PATH.joinpath('geo', 'sts_locations.geojson'))\n",
    "sts_locations = sts_locations.to_crs('EPSG:4326')\n",
    "sts_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1056it [02:04,  8.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "file_path = PATH.joinpath('processed', 'sts_tracks.parquet')\n",
    "\n",
    "for vessel in tqdm(PATH.joinpath('raw_gfw', 'tracks').rglob('*.zip')):\n",
    "    name = vessel.stem.split(' - ')[0]\n",
    "    zf = ZipFile(vessel)\n",
    "    for file in zf.namelist():\n",
    "        if 'csv' in file:\n",
    "            df = pd.read_csv(zf.open(file))\n",
    "            df.timestamp = pd.to_datetime(df.timestamp).dt.tz_localize(None)\n",
    "            df['name'] = name\n",
    "            gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=4326)\n",
    "            gdf = gdf.clip(sts_locations)\n",
    "            df = gdf.drop(['geometry', 'seg_id'], axis=1)\n",
    "\n",
    "            \n",
    "            if file_path.exists():\n",
    "                df.to_parquet(file_path, engine='fastparquet', append=True)\n",
    "            else:\n",
    "                df.to_parquet(file_path, engine='fastparquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create file of all tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for file in PATH.joinpath('raw_gfw', 'identity').glob('*.csv*'):\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs).reset_index(drop=True)\n",
    "df = df[['id', 'flag', 'mmsi', 'imo', 'shipname', 'callsign']].copy()\n",
    "df = df[df.imo.notna()].copy()\n",
    "df.imo = df.imo.astype(int)\n",
    "df.mmsi = df.mmsi.astype(int)\n",
    "df.shipname = df.shipname.str.upper().str.strip().str.replace(' ', '')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = PATH.joinpath('processed', 'tracks.parquet')\n",
    "\n",
    "for file in PATH.joinpath('raw_gfw', 'tracks').glob('*.zip'):\n",
    "    name = file.stem.split(' - ')[0].strip().upper().replace(' ', '')\n",
    "    zf = ZipFile(file)\n",
    "    for f in zf.namelist():\n",
    "        if f.endswith('.csv'):\n",
    "            vessel = pd.read_csv(zf.open(f))\n",
    "            vessel.drop('seg_id', axis=1, inplace=True)\n",
    "            vessel['name'] = name\n",
    "            vessel = pd.merge(vessel,\n",
    "                            df[['imo', 'shipname']],\n",
    "                            left_on='name',\n",
    "                            right_on='shipname',\n",
    "                            how='left')\n",
    "            vessel.drop('name', axis=1, inplace=True)\n",
    "            vessel.round({'lon': 3, 'lat': 3, 'course': 0, 'speed': 1})\n",
    "            vessel['timestamp'] = pd.to_datetime(vessel.timestamp)\n",
    "            vessel = vessel[vessel.timestamp >= '2022-01-01'].copy()\n",
    "            if file_path.exists():\n",
    "                vessel.to_parquet(file_path, engine='fastparquet', append=True)\n",
    "            else:\n",
    "                vessel.to_parquet(file_path, engine='fastparquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
