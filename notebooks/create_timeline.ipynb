{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from zipfile import ZipFile\n",
    "\n",
    "PATH = Path.cwd().parent.joinpath('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Create dataframe of a timeline with the following columns:\n",
    "0. Date\n",
    "1. IMO\n",
    "2. Event, which could be:\n",
    "    - name change\n",
    "    - flag change\n",
    "    - owner/manager change\n",
    "    - port visit\n",
    "    - loitering event\n",
    "    - ais gap\n",
    "    - sts area\n",
    "    - inspection\n",
    "3. Description: changed name from .. to .., changed flag from .. to .., changed owner from .. to .., visited port .., turned AIS off, turned AIS on, visited sts area, left sts area, inspected at, found x defeciencies and was/was not held in detention\n",
    "4. Type: flag_change, name_change, owner_change, ssvid_change, port_arrival, port_departure, loitering_start, loitering_stop, ais_off_switching, ais_on_switching, inspection. \n",
    "5. latitude\n",
    "6. longitude\n",
    "\n",
    "From 2022 onwards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(PATH.joinpath('processed', 'owners_names.csv'))\n",
    "names = names[['start_date', 'imo', 'vessel_name']].copy()\n",
    "names.rename(columns={'start_date': 'date'}, inplace=True)\n",
    "names['date'] = pd.to_datetime(names['date'])\n",
    "names.sort_values(by='date', ascending=False, inplace=True)\n",
    "names.reset_index(drop=True, inplace=True)\n",
    "names['previous_name'] = names.groupby('imo').vessel_name.shift(-1).fillna('UNKNOWN')\n",
    "names.query('date >= \"2022-01-01\"', inplace=True)\n",
    "names['description'] = names.apply(lambda row: f'Vessel name changed from {row.previous_name.upper()} to {row.vessel_name}', axis=1)\n",
    "names.drop(columns='previous_name', inplace=True)\n",
    "names['type'] = 'vessel_name_change'\n",
    "names.drop('vessel_name', axis=1, inplace=True)\n",
    "names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flag changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = pd.read_csv(PATH.joinpath('processed', 'owners_flags.csv'))\n",
    "flags.start_date = pd.to_datetime(flags.start_date)\n",
    "flags.sort_values(by='start_date', ascending=False, inplace=True)\n",
    "flags.reset_index(drop=True, inplace=True)\n",
    "flags.flag = flags.flag.str.replace('Not Known', 'UNKNOWN').str.upper()\n",
    "flags['previous_flag'] = flags.groupby('imo').flag.shift(-1).fillna('UNKNOWN')\n",
    "flags['description'] = flags.apply(lambda row: f'Vessel flag changed from {row.previous_flag.upper()} to {row.flag}', axis=1)\n",
    "flags = flags[['start_date', 'imo', 'flag', 'description']].copy()\n",
    "flags.rename(columns={'start_date': 'date'}, inplace=True)\n",
    "flags.query('date >= \"2022-01-01\"', inplace=True)\n",
    "flags['type'] = 'vessel_flag_change'\n",
    "flags.drop('flag', axis=1, inplace=True)\n",
    "flags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ownership changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners = pd.read_csv(PATH.joinpath('processed', 'owners_companies.csv'))\n",
    "owners.start_date = pd.to_datetime(owners.start_date)\n",
    "owners.sort_values(by='start_date', ascending=False, inplace=True)\n",
    "owners.reset_index(drop=True, inplace=True)\n",
    "owners['previous_owner'] = owners.groupby(['imo', 'role']).company.shift(-1).fillna('UNKNOWN')\n",
    "owners['description'] = owners.apply(lambda row: f'{row.role.upper()} changed from {row.previous_owner.upper()} to {row.company}', axis=1)\n",
    "owners = owners[['start_date', 'imo', 'company', 'description']].copy()\n",
    "owners.rename(columns={'start_date': 'date'}, inplace=True)\n",
    "owners.query('date >= \"2022-01-01\"', inplace=True)\n",
    "owners['type']= 'vessel_owner_change'\n",
    "owners.drop('company', axis=1, inplace=True)\n",
    "owners.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections = pd.read_csv(PATH.joinpath('processed', 'owners_inspections.csv'))\n",
    "inspections.date = pd.to_datetime(inspections.date)\n",
    "inspections.sort_values(by='date', ascending=False, inplace=True)\n",
    "inspections.reset_index(drop=True, inplace=True)\n",
    "inspections[['authority', 'port']] = inspections[['authority', 'port']].fillna('UNKNOWN')\n",
    "inspections['description'] = inspections.apply(lambda row: f'Vessel inspected in {row.port.upper()} in {row.authority.upper()} and {row.inspection_type} found {row.number_of_deficiencies} deficiencies. Detention={row.detention}', axis=1)\n",
    "inspections = inspections[['date', 'imo', 'description']].copy()\n",
    "inspections['type'] = 'vessel_inspection'\n",
    "inspections.query('date >= \"2022-01-01\"', inplace=True)\n",
    "inspections.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.concat([names, flags, owners, inspections], ignore_index=True).sort_values('date').reset_index(drop=True)\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data = pd.read_parquet(PATH.joinpath('processed', 'tracks.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add location data to dataframes\n",
    "\n",
    "def get_middle_timestamp(group):\n",
    "    sorted_group = group.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    middle_index = len(sorted_group) // 2\n",
    "    return sorted_group.loc[middle_index]\n",
    "\n",
    "def process_location_data(tracks, imo):\n",
    "    filtered_location_date = tracks.query(f'imo == {imo}').copy()\n",
    "    filtered_location_date['date'] = pd.to_datetime(filtered_location_date['timestamp']).dt.date\n",
    "    middle_timestamps = filtered_location_date.groupby(['imo', 'date']).apply(get_middle_timestamp, include_groups=False)\n",
    "\n",
    "    return middle_timestamps\n",
    "\n",
    "unique_imos = names.imo.unique()\n",
    "location_date_processed = pd.concat([process_location_data(location_data, imo) for imo in unique_imos])\n",
    "location_date_processed.reset_index(inplace=True)\n",
    "location_date_processed.date = pd.to_datetime(location_date_processed.date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.date = pd.to_datetime(events.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_date_processed['date'] = pd.to_datetime(location_date_processed['date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners.date = pd.to_datetime(owners.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(owners, \n",
    "                     location_date_processed[['timestamp', 'imo', 'lat', 'lon', 'date']], \n",
    "                     on=['imo', 'date'], \n",
    "                     how='left')\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data[(location_data.imo == 9327372) & (location_data.timestamp.dt.date == '2024-09-23')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop('timestamp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df.lat.isna()]['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Port visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = pd.read_parquet(PATH.joinpath('processed', 'ports.parquet'))\n",
    "ports_start = ports[['imo', 'start', 'lat', 'lon', 'port_visit_startAnchorage_id']].copy()\n",
    "ports_end = ports[['imo', 'end', 'lat', 'lon', 'port_visit_startAnchorage_id']].copy()\n",
    "ports_start.rename(columns={'start': 'date', 'port_visit_startAnchorage_id': 'port'}, inplace=True)\n",
    "ports_end.rename(columns={'end': 'date', 'port_visit_startAnchorage_id': 'port'}, inplace=True)\n",
    "ports_start['type'] = 'port_arrival'\n",
    "ports_end['type'] = 'port_departure'\n",
    "ports = pd.concat([ports_start, ports_end]).reset_index(drop=True)\n",
    "ports.query('date >= \"2022-01-01\"', inplace=True)\n",
    "ports['date'] = pd.to_datetime(ports.date).dt.date\n",
    "ports['description'] = ports.apply(lambda row: f'Vessel arrived to {row.port.upper()}' if row.type == 'port_arrival' else f'Vessel departed from {row.port.upper()}', axis=1)\n",
    "ports.drop(columns=['port'], inplace=True)\n",
    "ports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loitering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loitering = pd.read_parquet(PATH.joinpath('processed', 'loitering.parquet'))\n",
    "loitering_start = loitering[['imo', 'start', 'lat', 'lon']].copy()\n",
    "loitering_end = loitering[['imo', 'end', 'lat', 'lon']].copy()\n",
    "loitering_start.rename(columns={'start': 'date'}, inplace=True)\n",
    "loitering_end.rename(columns={'end': 'date'}, inplace=True)\n",
    "loitering_start['type'] = 'loitering_start'\n",
    "loitering_end['type'] = 'loitering_end'\n",
    "loitering = pd.concat([loitering_start, loitering_end]).reset_index(drop=True)\n",
    "loitering.query('date >= \"2022-01-01\"', inplace=True)\n",
    "loitering['date'] = pd.to_datetime(loitering.date).dt.date\n",
    "loitering['description'] = loitering.apply(lambda row: 'Vessel started loitering' if row.type == 'loitering_start' else 'Vessel stopped loitering', axis=1)\n",
    "loitering['type'] = 'loitering'\n",
    "#loitering.drop(columns=['lat', 'lon'], inplace=True)\n",
    "loitering.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIS gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais = pd.read_parquet(PATH.joinpath('processed', 'ais.parquet'))\n",
    "ais_start = ais[['imo', 'start', 'lat', 'lon']].copy()\n",
    "ais_end = ais[['imo', 'end', 'lat', 'lon']].copy()\n",
    "ais_start.rename(columns={'start': 'date'}, inplace=True)\n",
    "ais_end.rename(columns={'end': 'date'}, inplace=True)\n",
    "ais_start['type'] = 'possible_ais_off_switching'\n",
    "ais_end['type'] = 'possible_ais_on_switching'\n",
    "ais = pd.concat([ais_start, ais_end]).reset_index(drop=True)\n",
    "ais.query('date >= \"2022-01-01\"', inplace=True)\n",
    "ais['date'] = pd.to_datetime(ais.date).dt.date\n",
    "ais['description'] = ais.apply(lambda row: 'Vessel started broadcasting AIS' if row.type == 'ais_start' else 'Vessel stopped broadcasting AIS', axis=1)\n",
    "#ais.drop(columns=['lat', 'lon'], inplace=True)\n",
    "ais.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ship to ship transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = pd.read_parquet(PATH.joinpath('processed', 'sts_tracks.parquet'))\n",
    "sts = gpd.GeoDataFrame(sts, geometry=gpd.points_from_xy(sts.lon, sts.lat), crs='EPSG:4326')\n",
    "sts_locations = gpd.read_file(PATH.joinpath('geo', 'sts_locations.geojson'), crs='EPSG:4326')\n",
    "sts = gpd.sjoin(sts, sts_locations, how='left', predicate='within')\n",
    "sts.drop(['index_right', 'geometry'], axis=1, inplace=True)\n",
    "sts.rename(columns={'Name': 'sts_area'}, inplace=True)\n",
    "sts.name = sts.name.str.upper().str.replace(' ', '').str.strip()\n",
    "\n",
    "sts['time_diff_days'] = sts.groupby(['name', 'sts_area']).timestamp.diff().dt.total_seconds() / (60*60*24)\n",
    "sts['group'] = (sts.time_diff_days > 7).cumsum()\n",
    "sts['group'] = sts.groupby(['name', 'sts_area', 'group']).group.ffill().fillna(0).astype(int)\n",
    "sts.sort_values(by=['group', 'timestamp'], inplace=True)\n",
    "\n",
    "time_range = sts.groupby(['group', 'name']).agg(min_timestamp=('timestamp', 'min'), max_timestamp=('timestamp', 'max'))\n",
    "time_range = (time_range['max_timestamp'] - time_range['min_timestamp']).dt.total_seconds() / (60*60*24)\n",
    "sts = pd.merge(sts, time_range.reset_index(), on=['group', 'name'], how='left')\n",
    "sts.rename(columns={sts.columns[-1]: 'time_range'}, inplace=True)\n",
    "\n",
    "\n",
    "sts_start = sts.drop_duplicates(subset='group', keep='first').copy()\n",
    "sts_end = sts.drop_duplicates(subset='group', keep='last').copy()\n",
    "sts_start['type'] = 'possible_sts_start'\n",
    "sts_end['type'] = 'possible_sts_end'\n",
    "\n",
    "sts = pd.concat([sts_start, sts_end]).reset_index(drop=True)\n",
    "\n",
    "sts.sort_values(by=['group', 'timestamp'], inplace=True)\n",
    "sts.query('timestamp >= \"2022-01-01\" & time_range > 1', inplace=True)\n",
    "\n",
    "sts = sts[['timestamp', 'lat', 'lon', 'sts_area', 'name', 'type', 'group']].copy()\n",
    "sts.rename(columns={'timestamp': 'date'}, inplace=True)\n",
    "\n",
    "sts.date = pd.to_datetime(sts.date).dt.date\n",
    "sts['description'] = sts.apply(lambda row: f'Vessel {row[\"name\"]} entered {row.sts_area.upper()}' if row['type'] == 'sts_start' else f'Vessel {row[\"name\"]} exited {row.sts_area.upper()}', axis=1)\n",
    "\n",
    "sts.sort_values(by=['group', 'date'], inplace=True)\n",
    "sts.drop_duplicates(subset=['date', 'name', 'sts_area', 'type'], inplace=True)\n",
    "sts = sts[sts.groupby('group').group.transform('count') > 1].reset_index(drop=True)\n",
    "\n",
    "n = pd.read_csv(PATH.joinpath('processed', 'owners_names.csv'))\n",
    "n.vessel_name = n.vessel_name.str.upper().str.replace(' ', '').str.strip()\n",
    "\n",
    "sts = pd.merge(sts, \n",
    "                n[['vessel_name', 'imo']],\n",
    "                left_on='name',\n",
    "                right_on='vessel_name',\n",
    "                how='left')\n",
    "\n",
    "sts.dropna(subset='vessel_name', inplace=True)\n",
    "\n",
    "sts = sts[['date', 'imo', 'description', 'type']].copy()\n",
    "\n",
    "sts.imo = sts.imo.astype(int)\n",
    "sts.date = pd.to_datetime(sts.date).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.date = pd.to_datetime(events.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = pd.concat([events, ports, loitering, ais, sts], ignore_index=True).sort_values('date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline.to_csv(PATH.joinpath('db', 'timeline.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
