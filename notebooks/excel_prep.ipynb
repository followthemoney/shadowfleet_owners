{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import geopandas as gpd\n",
    "\n",
    "PATH = Path.cwd().parent.joinpath('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels = pd.read_csv(PATH.joinpath('processed', 'kse_shadowfleet.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [x for x in vessels.columns if '-202' in x]\n",
    "\n",
    "crude = vessels[vessels.commodity == 'crude'].copy()\n",
    "crude['crude_total'] = crude[cols].sum(axis=1)\n",
    "crude['months_uninsured'] = crude[cols].notna().sum(axis=1)\n",
    "\n",
    "products = vessels[vessels.commodity == 'oil products'].copy()\n",
    "products['products_total'] = products[cols].sum(axis=1)\n",
    "products['months_uninsured'] = products[cols].notna().sum(axis=1)\n",
    "\n",
    "crude = crude[['imo', 'vessel_name', 'tanker_size', 'buildyear', 'crude_total', 'months_uninsured']].copy()\n",
    "products = products[['imo', 'vessel_name', 'tanker_size', 'buildyear', 'products_total', 'months_uninsured']].copy()\n",
    "\n",
    "vessels = pd.merge(crude, \n",
    "                   products, \n",
    "                   on=['imo', 'vessel_name', 'tanker_size', 'buildyear'], how='outer')\n",
    "\n",
    "vessels.months_uninsured_x = vessels.months_uninsured_x.fillna(vessels.months_uninsured_y)\n",
    "vessels.drop('months_uninsured_y', axis=1, inplace=True)\n",
    "vessels.rename(columns={'months_uninsured_x': 'months_uninsured'}, inplace=True)\n",
    "vessels.months_uninsured = vessels.months_uninsured.astype(int)\n",
    "vessels = pd.merge(vessels,\n",
    "                   pd.read_csv(PATH.joinpath('processed', 'uninsured.csv')),\n",
    "                   on='imo', how='left')\n",
    "\n",
    "vessels.sort_values(by='earliest_sanction_date', inplace=True)\n",
    "vessels.drop_duplicates(subset='imo', keep='first', inplace=True)\n",
    "len(vessels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add ownership data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners = pd.read_csv(PATH.joinpath('processed', 'company_vessels_final.csv'))\n",
    "owners.fillna({'country': 'Unknown', 'end_date': '2022-01-01'}, inplace=True)\n",
    "owners.sort_values(by='start_date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = ['Registered owner', 'Ship manager / Commercial manager', 'ISM Manager']\n",
    "col_names = ['ownership', 'shipmanager', 'ism_manager']\n",
    "temp = []\n",
    "for r, n in zip(roles, col_names):\n",
    "\n",
    "    temp_owners = owners[owners.role==r].groupby('imo').size().reset_index()\n",
    "    temp_owners.columns = ['imo', f'{n}_changes_after_2022']\n",
    "    temp.append(temp_owners)\n",
    "\n",
    "    temp_owners = owners[owners.role==r]\\\n",
    "                         .groupby('imo')\\\n",
    "                         .agg({'country': lambda x: ', '.join(x)})\\\n",
    "                         .reset_index()\n",
    "    temp_owners.columns = ['imo', f'{n}_jurisdictions_after_2022']\n",
    "    temp.append(temp_owners)\n",
    "\n",
    "    ownership = reduce(lambda x, y: pd.merge(x, y, on='imo', how='outer'), temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels = pd.merge(vessels, ownership, on='imo', how='left')\n",
    "len(vessels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections = pd.read_csv(PATH.joinpath('processed', 'owners_inspections.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detentions_after_2022 = inspections[(inspections.detention=='Y') \\\n",
    "                                    & (inspections.date >='2022-01-01')]\\\n",
    "                                    .groupby('imo')\\\n",
    "                                    .size()\\\n",
    "                                    .reset_index()\\\n",
    "                                    .rename(columns={0: 'detentions_after_2022'})\n",
    "\n",
    "detentions = inspections[inspections.detention=='Y']\\\n",
    "                        .groupby('imo')\\\n",
    "                        .size()\\\n",
    "                        .reset_index()\\\n",
    "                        .rename(columns={0: 'detentions'})\n",
    "\n",
    "deficiencies_after_2022 = inspections[inspections.date >= '2022-01-01']\\\n",
    "                                       .groupby('imo')\\\n",
    "                                       .number_of_deficiencies\\\n",
    "                                       .size()\\\n",
    "                                       .reset_index()\\\n",
    "                                       .rename(columns={'number_of_deficiencies': 'deficiencies_after_2022'})\n",
    "\n",
    "deficiencies = inspections.groupby('imo')\\\n",
    "                            .number_of_deficiencies\\\n",
    "                            .size()\\\n",
    "                            .reset_index()\\\n",
    "                            .rename(columns={'number_of_deficiencies': 'deficiencies'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections = reduce(lambda x, y: pd.merge(x, y, on='imo', how='outer'), [detentions, detentions_after_2022, deficiencies, deficiencies_after_2022])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections.fillna(0, inplace=True)\n",
    "inspections.detentions_after_2022 = inspections.detentions_after_2022.astype(int)\n",
    "inspections.deficiencies_after_2022 = inspections.deficiencies_after_2022.astype(int)\n",
    "inspections.detentions = inspections.detentions.astype(int)\n",
    "inspections.deficiencies = inspections.deficiencies.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels = pd.merge(vessels, inspections, on='imo', how='left')\n",
    "len(vessels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = pd.read_csv(PATH.joinpath('processed', 'owners_names.csv'))\n",
    "name = name[name.start_date >= '2022-01-01'].copy()\n",
    "name_changes = name.groupby('imo').size().reset_index().rename(columns={0: 'name_changes_after_2022'})\n",
    "names = name.groupby('imo').agg({'vessel_name': lambda x: ', '.join(x)}).reset_index().rename(columns={'vessel_name': 'names_after_2022'})\n",
    "names = pd.merge(names, name_changes, on='imo', how='outer')\n",
    "\n",
    "vessels = pd.merge(vessels, names, on='imo', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = pd.read_csv(PATH.joinpath('processed', 'owners_flags.csv'))\n",
    "flags = flags[flags.start_date >= '2022-01-01'].copy()\n",
    "flag_changes = flags.groupby('imo').size().reset_index().rename(columns={0: 'flag_changes_after_2022'})\n",
    "flags = flags.groupby('imo').agg({'flag': lambda x: ', '.join(x)}).reset_index().rename(columns={'flag': 'flags_after_2022'})\n",
    "flags = pd.merge(flags, flag_changes, on='imo', how='outer')\n",
    "\n",
    "vessels = pd.merge(vessels, flags, on='imo', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add events to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais = pd.read_parquet(PATH.joinpath('processed', 'ais.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps = ais.groupby('imo').agg(ais_gaps=('imo', 'count'),\n",
    "                              ais_gap_total_hours=('gap_durationhours', 'sum')).reset_index()\n",
    "\n",
    "gaps_2022 = ais[ais.start >= '2022-01-01']\\\n",
    "                .groupby('imo')\\\n",
    "                .agg(ais_gaps_after_2022=('imo', 'count'),\n",
    "                     ais_gap_total_hours_after_2022=('gap_durationhours', 'sum'))\\\n",
    "                .reset_index()\n",
    "\n",
    "gaps = pd.merge(gaps, gaps_2022, on='imo', how='outer')\n",
    "\n",
    "gaps.ais_gaps_after_2022 = gaps.ais_gaps_after_2022.fillna(0).astype(int)\n",
    "gaps.ais_gaps = gaps.ais_gaps.fillna(0).astype(int)\n",
    "gaps.ais_gap_total_hours = round(gaps.ais_gap_total_hours.fillna(0).astype(int))\n",
    "gaps.ais_gap_total_hours_after_2022 = round(gaps.ais_gap_total_hours_after_2022.fillna(0).astype(int))\n",
    "gaps.imo = gaps.imo.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = pd.read_parquet(PATH.joinpath('processed', 'ports.parquet'))\n",
    "ports.start = pd.to_datetime(ports.start).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['RUS', 'TUR', 'IND']\n",
    "temps = []\n",
    "for country in countries:\n",
    "    temp_2022 = ports[(ports.start >= '2022-01-01') & (ports.port_visit_startAnchorage_flag == country)].groupby('imo').size().reset_index()\n",
    "    temp_2022.rename({0: f'{country}_port_visits_after_2022'}, axis=1, inplace=True)\n",
    "    temps.append(temp_2022)\n",
    "    temp = ports[(ports.start < '2022-01-01') & (ports.start >= '2019-02-01') & (ports.port_visit_startAnchorage_flag == country)].groupby('imo').size().reset_index()\n",
    "    temp.rename({0: f'{country}_port_visits_before_2022'}, axis=1, inplace=True)\n",
    "    temps.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = reduce(lambda x, y: pd.merge(x, y, on='imo', how='outer'), temps)\n",
    "visits.fillna(0, inplace=True)\n",
    "visits[[col for col in visits.columns]] = visits[[col for col in visits.columns]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loitering = pd.read_parquet(PATH.joinpath('processed', 'loitering.parquet'))\n",
    "loitering.start = pd.to_datetime(loitering.start).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = loitering[(loitering.start < '2022-01-01') & (loitering.start >= '2019-02-01')].groupby('imo').size().reset_index().rename(columns={0: 'loitering_events_before_2022'})\n",
    "temp_2022 = loitering[loitering.start >= '2022-01-01'].groupby('imo').size().reset_index().rename(columns={0: 'loitering_events_after_2022'})\n",
    "\n",
    "loitering = pd.merge(temp, temp_2022, on='imo', how='outer')\n",
    "loitering.fillna(0, inplace=True)\n",
    "loitering.loitering_events_before_2022 = loitering.loitering_events_before_2022.astype(int)\n",
    "loitering.loitering_events_after_2022 = loitering.loitering_events_after_2022.astype(int)\n",
    "loitering.imo = loitering.imo.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = pd.read_parquet(PATH.joinpath('processed', 'sts_tracks.parquet'))\n",
    "sts = gpd.GeoDataFrame(sts, geometry=gpd.points_from_xy(sts.lon, sts.lat), crs='EPSG:4326')\n",
    "sts_locations = gpd.read_file(PATH.joinpath('geo', 'sts_locations.geojson'), crs='EPSG:4326')\n",
    "sts = gpd.sjoin(sts, sts_locations, how='left', predicate='within')\n",
    "sts.drop(['index_right', 'geometry'], axis=1, inplace=True)\n",
    "sts.rename(columns={'Name': 'sts_area'}, inplace=True)\n",
    "sts.name = sts.name.str.upper().str.replace(' ', '').str.strip()\n",
    "\n",
    "sts['time_diff_days'] = sts.groupby(['name', 'sts_area']).timestamp.diff().dt.total_seconds() / (60*60*24)\n",
    "sts['group'] = (sts.time_diff_days > 7).cumsum()\n",
    "sts['group'] = sts.groupby(['name', 'sts_area', 'group']).group.ffill().fillna(0).astype(int)\n",
    "sts.sort_values(by=['group', 'timestamp'], inplace=True)\n",
    "\n",
    "time_range = sts.groupby(['group', 'name']).agg(min_timestamp=('timestamp', 'min'), max_timestamp=('timestamp', 'max'))\n",
    "time_range = (time_range['max_timestamp'] - time_range['min_timestamp']).dt.total_seconds() / (60*60*24)\n",
    "sts = pd.merge(sts, time_range.reset_index(), on=['group', 'name'], how='left')\n",
    "sts.rename(columns={sts.columns[-1]: 'time_range'}, inplace=True)\n",
    "\n",
    "\n",
    "sts_start = sts.drop_duplicates(subset='group', keep='first').copy()\n",
    "sts_end = sts.drop_duplicates(subset='group', keep='last').copy()\n",
    "sts_start['type'] = 'possible_sts_start'\n",
    "sts_end['type'] = 'possible_sts_end'\n",
    "\n",
    "sts = pd.concat([sts_start, sts_end]).reset_index(drop=True)\n",
    "\n",
    "sts.sort_values(by=['group', 'timestamp'], inplace=True)\n",
    "sts.query('timestamp >= \"2022-01-01\" & time_range > 1', inplace=True)\n",
    "\n",
    "sts = sts[['timestamp', 'lat', 'lon', 'sts_area', 'name', 'type', 'group']].copy()\n",
    "sts.rename(columns={'timestamp': 'date'}, inplace=True)\n",
    "\n",
    "sts.date = pd.to_datetime(sts.date).dt.date\n",
    "sts['description'] = sts.apply(lambda row: f'Vessel {row[\"name\"]} entered {row.sts_area.upper()}' if row['type'] == 'sts_start' else f'Vessel {row[\"name\"]} exited {row.sts_area.upper()}', axis=1)\n",
    "\n",
    "sts.sort_values(by=['group', 'date'], inplace=True)\n",
    "sts.drop_duplicates(subset=['date', 'name', 'sts_area', 'type'], inplace=True)\n",
    "sts = sts[sts.groupby('group').group.transform('count') > 1].reset_index(drop=True)\n",
    "\n",
    "n = pd.read_csv(PATH.joinpath('processed', 'owners_names.csv'))\n",
    "n.vessel_name = n.vessel_name.str.upper().str.replace(' ', '').str.strip()\n",
    "\n",
    "sts = pd.merge(sts, \n",
    "                n[['vessel_name', 'imo']],\n",
    "                left_on='name',\n",
    "                right_on='vessel_name',\n",
    "                how='left')\n",
    "\n",
    "sts.dropna(subset='vessel_name', inplace=True)\n",
    "\n",
    "sts = sts[['date', 'imo', 'description', 'type']].copy()\n",
    "\n",
    "sts.imo = sts.imo.astype(int)\n",
    "sts.date = pd.to_datetime(sts.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_events = sts.groupby('imo').size().reset_index().rename(columns={0: 'sts_events'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.to_csv(PATH.joinpath('db', 'sts_events.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [gaps, visits, loitering, sts_events]\n",
    "events = reduce(lambda x, y: pd.merge(x, y, on='imo', how='outer'), events)\n",
    "vessels = pd.merge(vessels, events, on='imo', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ownership_changes_after_2022', 'shipmanager_changes_after_2022', 'name_changes_after_2022',\n",
    "         'flag_changes_after_2022',\n",
    "       'ism_manager_changes_after_2022', 'detentions',\n",
    "       'detentions_after_2022', 'deficiencies', 'deficiencies_after_2022',\n",
    "       'ais_gaps', 'ais_gap_total_hours', 'ais_gaps_after_2022',\n",
    "       'ais_gap_total_hours_after_2022', 'RUS_port_visits_after_2022',\n",
    "       'RUS_port_visits_before_2022', 'TUR_port_visits_after_2022',\n",
    "       'TUR_port_visits_before_2022', 'IND_port_visits_after_2022',\n",
    "       'IND_port_visits_before_2022', 'loitering_events_before_2022',\n",
    "       'loitering_events_after_2022', 'sts_events']\n",
    "\n",
    "\n",
    "\n",
    "vessels[cols] = vessels[cols].fillna(0).astype(int)\n",
    "vessels.fillna({'products_total': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels.to_csv(PATH.joinpath('db', 'for_inspection.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
