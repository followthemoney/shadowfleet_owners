{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shadowfleet as sf\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon\n",
    "import plotly.express as px\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "PATH = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import manually extracted vessel presence from terminals and locations of interest\n",
    "dfs = []\n",
    "\n",
    "for file in PATH.joinpath('oil_flows', 'visits').glob('*.json'):\n",
    "    records = json.load(file.open()).get('entries')\n",
    "    for record in records:\n",
    "        r = record.get('public-global-presence:v3.0')\n",
    "        df = pd.DataFrame(r)\n",
    "        df['terminal'] = file.stem.split(' ')[0]\n",
    "        dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Filter out possible passerby vessels. Adapt min_hours to your needs\n",
    "\n",
    "min_hours = 8\n",
    "df.query(f'hours >= {min_hours}', inplace=True)\n",
    "\n",
    "\n",
    "# Clean\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df.drop(['entryTimestamp', 'exitTimestamp'], axis=1, inplace=True)\n",
    "df.sort_values(['mmsi', 'date'], inplace=True)\n",
    "df.query('mmsi.notnull() & mmsi != \"\"', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.mmsi = df.mmsi.astype(int)\n",
    "\n",
    "# Calculate the difference in days between the current date and the previous date for each 'mmsi'\n",
    "df['days_diff'] = df.groupby('mmsi')['date'].diff().dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vessel and owner information from GFW - uncomment to run\n",
    "'''\n",
    "vessels, owners = sf.get_vessels(query=df['mmsi'].unique()[2598:], \n",
    "                                 filename=PATH.joinpath('oil_flows', 'vessels.json'),\n",
    "                                 limit=5, \n",
    "                                 field='mmsi')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vessel info\n",
    "\n",
    "rows = []\n",
    "with open(PATH.joinpath('oil_flows', 'vessels.json'), 'r') as file:\n",
    "    for row in file:\n",
    "        record = literal_eval(row).get('entries')[0].get('selfReportedInfo')[0]\n",
    "        rows.append(record)\n",
    "with open(PATH.joinpath('oil_flows', 'vessels2.json'), 'r') as file:\n",
    "    for row in file:\n",
    "        record  = literal_eval(row).get('entries')[0].get('selfReportedInfo')[0]\n",
    "        rows.append(record)\n",
    "\n",
    "vessels = pd.DataFrame(rows).drop_duplicates('id')\n",
    "vessels = vessels[vessels.ssvid.notnull()].copy()\n",
    "vessels.ssvid = vessels.ssvid.astype(int)\n",
    "vessels.rename(columns={'ssvid': 'mmsi'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, \n",
    "              vessels[['mmsi', 'imo', 'id', 'shipname', 'flag']], \n",
    "              on='mmsi', \n",
    "              how='left')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get port visits from GFW - uncomment to run\n",
    "\n",
    "'''\n",
    "dfs = []\n",
    "for id in df.id.unique():\n",
    "    port_visits = sf.get_events(vessel_id=id, \n",
    "                                event_type='port_visits', \n",
    "                                filename=PATH.joinpath('oil_flows', 'port_visits.json'),\n",
    "                                start_date='2022-01-01',\n",
    "                                end_date='2024-11-01'\n",
    "                                )\n",
    "    dfs.append(port_visits)\n",
    "\n",
    "port_visits = pd.concat(dfs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import port visits\n",
    "rows = []\n",
    "with open(PATH.joinpath('oil_flows', 'port_visits.json'), 'r') as file:\n",
    "     for row in file:\n",
    "         try:\n",
    "            if len(literal_eval(row).get('entries')) > 0:\n",
    "               for record in literal_eval(row).get('entries'):\n",
    "                   vessel = record.get('vessel')\n",
    "                   anchorage = record.get('port_visit').get('endAnchorage')\n",
    "                   rows.append({'start': record.get('start'),\n",
    "                                'end': record.get('end'),\n",
    "                                'visit_id': record.get('id'),\n",
    "                                'vessel_id': vessel.get('id'),\n",
    "                                'mmsi': vessel.get('ssvid'),\n",
    "                                'name': vessel.get('name'),\n",
    "                                'flag': vessel.get('flag'),\n",
    "                                'port': anchorage.get('name'),\n",
    "                                'port_country': anchorage.get('flag'),\n",
    "                                'lon': anchorage.get('lon'),\n",
    "                                'lat': anchorage.get('lat'),\n",
    "                                'duration': round(record.get('port_visit').get('durationHrs'), 1)\n",
    "                                })\n",
    "         except ValueError:\n",
    "            print(f'could not parse row')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = pd.DataFrame(rows)\n",
    "len(pv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv[['start' ,'end']] = pv[['start', 'end']].apply(pd.to_datetime)\n",
    "pv.mmsi = pv.mmsi.astype(int)\n",
    "pv.to_csv(PATH.joinpath('oil_flows', 'port_visits.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = pd.read_csv(PATH.joinpath('oil_flows', 'port_visits.csv'))\n",
    "pv[['start' ,'end']] = pv[['start', 'end']].apply(pd.to_datetime)\n",
    "pv.mmsi = pv.mmsi.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df and port_visits on 'mmsi'\n",
    "merged_df = pd.merge(pv, \n",
    "                     df[['date', 'mmsi', 'terminal',]], \n",
    "                     on='mmsi', \n",
    "                     how='left')\n",
    "\n",
    "\n",
    "\n",
    "# Filter rows where the date of df falls between the start and end date of port_visits\n",
    "filtered_df = merged_df[(merged_df['date'] >= merged_df['start'].dt.date) & (merged_df['date'] <= merged_df['end'].dt.date)].copy()\n",
    "\n",
    "filtered_df.drop_duplicates(inplace=True)\n",
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_filtered = pv[~pv.visit_id.isin(filtered_df.visit_id)].copy()\n",
    "visits = pd.concat([filtered_df, pv_filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.sort_values(['mmsi', 'date'], inplace=True)\n",
    "visits.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the terminal values of interest\n",
    "country_of_interest = ['IND']\n",
    "country = ['NLD']\n",
    "\n",
    "# Initialize an empty list to store the rows of interest\n",
    "rows_of_interest = []\n",
    "\n",
    "# Group the dataframe by 'mmsi' to process each vessel separately\n",
    "grouped = visits.groupby('mmsi')\n",
    "\n",
    "# Iterate through each group\n",
    "for mmsi, group in grouped:\n",
    "    # Sort the group by the 'start' column to ensure chronological order\n",
    "    group = group.sort_values('start')\n",
    "    \n",
    "    # Iterate through the rows of the group\n",
    "    for i in range(len(group)):\n",
    "        current_terminal = group.iloc[i]['port_country']\n",
    "        \n",
    "        # Check if the current terminal is in the terminals of interest\n",
    "        if current_terminal in country_of_interest:\n",
    "            # Check the previous row if it exists\n",
    "            if i > 0:\n",
    "                previous_terminal = group.iloc[i - 1]['port_country']\n",
    "                if previous_terminal in country:\n",
    "                    rows_of_interest.append(group.iloc[i])\n",
    "                    rows_of_interest.append(group.iloc[i - 1])\n",
    "            \n",
    "            # Check the next row if it exists\n",
    "            if i < len(group) - 1:\n",
    "                next_terminal = group.iloc[i + 1]['terminal']\n",
    "                if next_terminal in country:\n",
    "                    rows_of_interest.append(group.iloc[i])\n",
    "                    rows_of_interest.append(group.iloc[i + 1])\n",
    "\n",
    "# Create a new dataframe from the rows of interest\n",
    "result_df = pd.DataFrame(rows_of_interest).drop_duplicates()\n",
    "\n",
    "# Display the resulting dataframe\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count NaN values in each row\n",
    "def count_nans(row):\n",
    "    return row.isna().sum()\n",
    "\n",
    "# Add a temporary column to count NaN values\n",
    "visits['nan_count'] = visits.apply(count_nans, axis=1)\n",
    "\n",
    "# Sort by 'nan_count' and drop duplicates, keeping the first occurrence (which has the least NaN values)\n",
    "visits = visits.sort_values('nan_count').drop_duplicates(subset=['mmsi', 'date'], keep='first')\n",
    "\n",
    "# Drop the temporary 'nan_count' column\n",
    "visits.drop(columns=['nan_count'], inplace=True)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search by geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import polygons of terminals\n",
    "\n",
    "terminals = gpd.read_file(PATH.joinpath('geo', 'oil_terminals.geojson'))\n",
    "terminals.crs = 'EPSG:4326'\n",
    "\n",
    "# Convert LineStrings to Polygons\n",
    "terminals['geometry'] = terminals['geometry'].apply(lambda geom: Polygon(geom) if geom.is_valid else None)\n",
    "\n",
    "# Drop any rows where the geometry conversion failed\n",
    "terminals = terminals.dropna(subset=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get port visits from GFW\n",
    "\n",
    "geometries = terminals.geometry.__geo_interface__.get('features')\n",
    "terminal_list = terminals.name.to_list()\n",
    "dfs = []\n",
    "\n",
    "for geom, terminal in zip(geometries, terminal_list):\n",
    "    geom = geom.get('geometry')\n",
    "\n",
    "    df = sf.get_events_by_geometry(start_date = '2022-01-01',\n",
    "                            end_date = '2024-11-01',\n",
    "                            event_type='port_visits',\n",
    "                            geometry = geom,\n",
    "                            filename = PATH.joinpath('oil_flows', 'port_visits_oil_terminals.json'))\n",
    "    df['terminal'] = terminal\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "terminal_visits = pd.concat(dfs)\n",
    "\n",
    "len(terminal_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all port visits of these vessels\n",
    "\n",
    "visits = []\n",
    "for id in vessels.id.unique():\n",
    "    port_visit = sf.get_events(vessel_id=id,\n",
    "                               start_date='2022-01-01',\n",
    "                               end_date='2024-11-01',\n",
    "                               event_type='port_visits',\n",
    "                               filename=PATH.joinpath('oil_flows', 'prev_visits.json'))\n",
    "    visits.append(port_visit)\n",
    "\n",
    "prev_visits = pd.concat(visits)\n",
    "len(prev_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "\n",
    "terminal_visits.to_csv(PATH.joinpath('oil_flows', 'port_visits_oil_terminals.csv'), index=False)\n",
    "vessels.to_csv(PATH.joinpath('oil_flows', 'vessels.csv'), index=False)\n",
    "prev_visits.to_csv(PATH.joinpath('oil_flows', 'previous_port_visits.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminals.to_file(PATH.joinpath('geo', 'oil_terminals.geojson'), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "terminal_visits = pd.read_csv(PATH.joinpath('oil_flows', 'port_visits_oil_terminals.csv'))\n",
    "prev_visits = pd.read_csv(PATH.joinpath('oil_flows', 'previous_port_visits.csv'))\n",
    "vessels = pd.read_csv(PATH.joinpath('oil_flows', 'vessels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up columns\n",
    "\n",
    "cols = ['start', 'end', 'id', 'vessel.id', 'vessel.flag', 'vessel.ssvid', 'vessel.name', 'vessel.type', \n",
    "        'port_visit.durationHrs', 'position.lat', 'port_visit.startAnchorage.id', \n",
    "        'port_visit.intermediateAnchorage.id', 'port_visit.endAnchorage.id', 'position.lon', 'terminal']\n",
    "\n",
    "terminal_visits = terminal_visits[cols].copy()\n",
    "prev_visits = prev_visits[cols[:-1]].copy()\n",
    "\n",
    "renamed = {'port_visit.durationHrs': 'duration_hrs',\n",
    "           'vessel.id': 'vessel_id',\n",
    "           'port_visit.intermediateAnchorage.id': 'intermediate_anchorage_id',\n",
    "           'port_visit.startAnchorage.anchorageId': 'start_anchorage_id',\n",
    "           'port_visit.endAnchorage.id': 'end_anchorage_id',\n",
    "           'position.lat': 'lat',\n",
    "           'position.lon': 'lon'\n",
    "          }\n",
    "\n",
    "for key, value in renamed.items():\n",
    "    cols = [x.replace(key, value) for x in cols]\n",
    "\n",
    "cols = [col.replace('vessel.', '') for col in cols]\n",
    "\n",
    "terminal_visits.columns = cols\n",
    "prev_visits.columns = cols[:-1]\n",
    "\n",
    "# Drop rows with irrelevant vessel types\n",
    "terminal_visits = terminal_visits[~terminal_visits['type'].isin(['fishing', 'passenger', 'gear', 'seismic_vessel'])].copy()\n",
    "prev_visits = prev_visits[~prev_visits['type'].isin(['fishing', 'passenger', 'gear', 'seismic_vessel'])].copy()\n",
    "\n",
    "# Add IMO number to vessels\n",
    "prev_visits = pd.merge(prev_visits, vessels[vessels.imo.notna()][['ssvid', 'imo']], on='ssvid', how='left')\n",
    "prev_visits.dropna(subset=['imo'], inplace=True)\n",
    "prev_visits.imo = prev_visits.imo.astype(int)\n",
    "\n",
    "# Sort values\n",
    "prev_visits.sort_values(['ssvid', 'start'], inplace=True)\n",
    "\n",
    "# Add terminal visit column\n",
    "prev_visits = pd.merge(prev_visits, \n",
    "                        terminal_visits[['id', 'terminal']], \n",
    "                        on='id', how='left')\n",
    "\n",
    "len(prev_visits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_visits[(terminal_visits.terminal.notna()) & (terminal_visits.terminal.str.contains('Gunvor'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_visits = pd.merge(prev_visits, terminal_visits[['terminal', 'id']], on='id', how='left')\n",
    "prev_visits.drop_duplicates(subset='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_visits.start = pd.to_datetime(prev_visits.start)\n",
    "prev_visits.sort_values(['ssvid', 'start'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the rows of interest\n",
    "rows_of_interest = []\n",
    "\n",
    "# Group the dataframe by 'ssvid' to process each vessel separately\n",
    "grouped = prev_visits.groupby('ssvid')\n",
    "\n",
    "# Iterate through each group\n",
    "for ssvid, group in grouped:\n",
    "    # Sort the group by the 'start' column to ensure chronological order\n",
    "    group = group.sort_values('start')\n",
    "    \n",
    "    # Find the indices where the vessel was in a terminal of interest\n",
    "    terminal_indices = group[group['terminal'].notna()].index\n",
    "    \n",
    "    for idx in terminal_indices:\n",
    "        # Get the index of the current row\n",
    "        current_idx = group.index.get_loc(idx)\n",
    "        \n",
    "        # Get the previous and next rows if they exist\n",
    "        if current_idx > 0:\n",
    "            rows_of_interest.append(group.iloc[current_idx - 1])\n",
    "        rows_of_interest.append(group.iloc[current_idx])\n",
    "        if current_idx < len(group) - 1:\n",
    "            rows_of_interest.append(group.iloc[current_idx + 1])\n",
    "\n",
    "# Create a new dataframe from the rows of interest\n",
    "port_visits_before_after = pd.DataFrame(rows_of_interest)\n",
    "\n",
    "# Display the new dataframe\n",
    "port_visits_before_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_visits.query('terminal == \"Eurotank\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_visits_before_after.query('ssvid == 202509374')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
